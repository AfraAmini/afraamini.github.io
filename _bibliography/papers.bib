---
---
@inproceedings{amini2023hexatagging,
      title={Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective},
      author={Tianyu Liu and Afra Amini and Mrinmaya Sachan and Ryan Cotterell},
      year={2023},
      abbr={EMNLP 2023},
      selected=true,
      award={Outstanding Paper Award},
      booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
      publisher = {Association for Computational Linguistics},
      pdf = {https://arxiv.org/pdf/2306.05477.pdf},
      abstract = {Tasks that model the relation between pairs of tokens in a string are a vital part of understanding natural language. Such tasks, in general, require exhaustive pair-wise comparisons of tokens, thus having a quadratic runtime complexity in the length of the string. We show that these exhaustive comparisons can be avoided, and, moreover, the complexity of such tasks can be reduced to linear by casting the relation between tokens as a partial order over the string. Our method predicts real numbers for each token in a string in parallel and sorts the tokens accordingly, resulting in total orders of the tokens in the string. Each total order implies a set of arcs oriented from smaller to greater tokens, sorted by their predicted numbers. The intersection of total orders results in a partial order over the set of tokens in the string, which is then decoded into a directed graph representing the desired linguistic structure. Our experiments on dependency parsing and coreference resolution show that our method achieves state-of-the-art or comparable performance. Moreover, the linear complexity and parallelism of our method double the speed of graph-based coreference resolution models, and bring a 10-times speed-up over graph-based dependency parsers.},
}

@inproceedings{amini2023structured,
      title={Structured Voronoi Sampling},
      author={Afra Amini and Li Du and Ryan Cotterell},
      year={2023},
      booktitle = {Proceedings Neural Information Processing Systems (NeurIPS)},
      pdf = {https://arxiv.org/pdf/2306.03061.pdf},
      abstract = {Recently, there has been a growing interest in the development of gradient-based sampling algorithms for text generation, especially in the context of controlled generation. However, there exists a lack of theoretically grounded and principled approaches for this task. In this paper, we take an important step toward building a principled approach for sampling from language models with gradient-based methods. We use discrete distributions given by language models to define densities and develop an algorithm based on Hamiltonian Monte Carlo to sample from them. We name our gradient-based technique Structured Voronoi Sampling (SVS). In an experimental setup where the reference distribution is known, we show that the empirical distribution of SVS samples is closer to the reference distribution compared to alternative sampling schemes. Furthermore, in a controlled generation task, SVS is able to generate fluent and diverse samples while following the control targets significantly better than other methods.},
      abbr={NeurIPS 2023},
      selected=true,
}

@inproceedings{amini2023hexatagging,
      title={Hexatagging: Projective Dependency Parsing as Tagging},
      author={Afra Amini* and Tianyu Liu* and Ryan Cotterell},
      year={2023},
      abbr={ACL 2023},
      selected=true,
      award={Outstanding Paper Award},
      booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Short Papers},
      publisher = {Association for Computational Linguistics},
      pdf = {https://arxiv.org/pdf/2306.05477.pdf},
      abstract = {We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parserâ€™s linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-up over previous state-of-the-art models during decoding.}
}

@inproceedings{demonli,
      title={Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals},
      author={Robin Chan and Afra Amini and Mennatallah El-Assady},
      year={2023},
      abbr={ACL 2023},
      selected=true,
      booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
      publisher = {Association for Computational Linguistics},
      pdf = {https://arxiv.org/pdf/2306.12146.pdf},
      abstract = {We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback. Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models' robustness by creating adversarial test suites.}
}

@inproceedings{amini-etal-2022-pat,
    title = {On Parsing as Tagging},
    author = "Amini, Afra  and
      Cotterell, Ryan",
    abbr={EMNLP 2022},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    year = {2022},
    publisher = {Association for Computational Linguistics},
    pdf = {https://arxiv.org/pdf/2211.07344.pdf},
    arxiv = {2211.07344},
    selected=true,
    abstract = {There have been many proposals to reduce constituency parsing to tagging in the literature. To better understand what these approaches have in common, we cast several existing proposals into a unifying pipeline consisting of three steps: linearization, learning, and decoding. In particular, we show how to reduce tetratagging, a state-of-the-art constituency tagger, to shift--reduce parsing by performing a right-corner transformation on the grammar and making a specific independence assumption. Furthermore, we empirically evaluate our taxonomy of tagging pipelines with different choices of linearizers, learners, and decoders. Based on the results in English and a set of 8 typologically diverse languages, we conclude that the linearization of the derivation tree and its alignment with the input sequence is the most critical factor in achieving accurate taggers.}
}

@article{amini2022naturalistic,
  title={Naturalistic Causal Probing for Morpho-Syntax},
  abbr={TACL 2022},
  author={Amini, Afra and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan},
  journal={Transactions of the Association for Computational Linguistics},
  pdf={https://arxiv.org/pdf/2205.07043.pdf},
  arxiv={2205.07043},
  selected=true,
  year={2022},
  abstract = {Probing has become a go-to methodology for interpreting and analyzing deep neural models in natural language processing. Yet recently, there has been much debate around the limitations and weaknesses of probes. In this work, we suggest a naturalistic strategy for input-level intervention on real world data in Spanish, which is a language with gender marking. Using our approach, we isolate morpho-syntactic features from counfounders in sentences, e.g. topic, which will then allow us to causally probe pre-trained models. We apply this methodology to analyze causal effects of gender and number on contextualized representations extracted from pre-trained models -- BERT, RoBERTa and GPT-2. Our experiments suggest that naturalistic intervention can give us stable estimates of causal effects, which varies across different words in a sentence. We further show the utility of our estimator in investigating gender bias in adjectives, and answering counterfactual questions in masked prediction. Our probing experiments highlights the importance of conducting causal probing in determining if a particular property is encoded in representations.},
}

@inproceedings{meister-etal-2021-conditional,
    title = {Conditional {P}oisson Stochastic Beams},
    author = "Meister, Clara  and
      Amini, Afra  and
      Vieira, Tim  and
      Cotterell, Ryan",
    abbr={EMNLP 2021},
    booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
    year = {2021},
    address = {Online and Punta Cana, Dominican Republic},
    publisher = {Association for Computational Linguistics},
    pdf = {https://aclanthology.org/2021.emnlp-main.52.pdf},
    acl = {https://aclanthology.org/2021.emnlp-main.52},
    arxiv = {2109.11034},
    doi = {10.18653/v1/2021.emnlp-main.52},
    pages = {664--681},
    selected=true,
    abstract = {Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019){'}s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.},
}

